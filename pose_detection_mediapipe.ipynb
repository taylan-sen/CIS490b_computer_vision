{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylan-sen/CIS490b_computer_vision/blob/main/pose_detection_mediapipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6RCyy6NXnaO"
      },
      "source": [
        "# Human pose detection using MediaPipe\n",
        "MediaPipe is a powerful set of tools developed by Google, allowing to apply diversified tasks linked to computer vision (*eg* object detection, gesture recognition), text (*eg* text embedding) and audio data (audio classification). In this tutorial, I am going to show you how to use MediaPipe to detect pose landmarks in videos, following these steps:\n",
        "\n",
        "\n",
        "*   Import and install the necessary Python packages\n",
        "*   Download the MediaPipe pose landmark detection model\n",
        "*   Load an input video\n",
        "*   Extract the pose landmarks on each video frame, and visualize them\n",
        "*   Save the current frame alongside with the extracted pose landmarks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr1Xj4CRenAo"
      },
      "source": [
        "## Required packages\n",
        "\n",
        "To be able to follow this notebook, these packages should be installed:\n",
        "\n",
        "\n",
        "*   MediaPipe\n",
        "*   OpenCV\n",
        "*   NumPy\n",
        "\n",
        "This tutorial does not cover opencv and numpy installation (you can visit these links to install [NumPy](https://numpy.org/install/) and [OpenCV](https://pypi.org/project/opencv-python/)). You can get MediaPipe installed using:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4kfsaiIgxVgE",
        "outputId": "ae385bce-c5e4-461b-8f7a-4441a9b629b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install MediaPipe\n",
        "!pip install -q mediapipe==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8EhkSxrhmMR"
      },
      "source": [
        "Let's begin with importing the above-mentioned packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tf5wqP6oPlEu"
      },
      "outputs": [],
      "source": [
        "# Import the required packages\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY4NsBdipGwz"
      },
      "source": [
        "If you are running this notebook on Google Colab, and need to access files in your Google Drive, you will need to mount Google Drive. This can be achieved with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h3DJiQ1Myoia",
        "outputId": "b2cb78e6-bd83-4b9c-bcd6-a93ad7e0cc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0f6d3514c078>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pose landmark detection model\n",
        "\n",
        "The pose landmarker model proposed by MediaPipe allows to extract 33 body landmark 3D coordinates (the landmarks are shown in the figure below), estimate for each landmark whether it is visible in the input frame (meaning not hidden by another body part or by an object), and indicate the probability that it is present in the frame (i.e. inside the frame).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/rmeziatisab/PoseDetection/blob/main/pose_landmarks_index.png?raw=true:, width=250\" alt=\"My Image\" width=250>\n",
        "  <p style=\"font-size: 14px;\" align=\"center\">\n",
        "      <i>MediaPipe pose landmarks</i> *.\n",
        "  </p>\n",
        "</p>\n",
        "\n",
        "You can download the pose landmarker model [here](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models). For this tutorial, I will use the Full model.<!-- Here is another option to get the pose landmarker model:\n",
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_full.task\n",
        "-->\n",
        "\n",
        "<p id=\"landmarks_img_src\" style=\"fint-size: 10px;\">\n",
        "   <i>* <a href=\"https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#pose_landmarker_model\">Source</a></i>\n",
        "</p>"
      ],
      "metadata": {
        "id": "ha9LBirdYKan"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOxiFggOxQvL"
      },
      "source": [
        "## Pose landmark visualization\n",
        "For body landmark visualization, I will be using the following function, developed by the MediaPipe authors. This function will be called to annotate the current frame with the detected body landmarks, as well as draw the connections between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YurAWzW-RAbP"
      },
      "outputs": [],
      "source": [
        "# Use function allowing to draw pose landmarks on an image\n",
        "from mediapipe.python import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    pose_landmarks_list = detection_result.pose_landmarks\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        pose_landmarks_proto.landmark.extend([\n",
        "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "        ])\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            pose_landmarks_proto,\n",
        "            solutions.pose.POSE_CONNECTIONS,\n",
        "            solutions.drawing_styles.get_default_pose_landmarks_style()\n",
        "        )\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_6Nuekp3pGL"
      },
      "source": [
        "## Pose landmark extraction\n",
        "The first thing to do is load the video the pose detection model will be applied to. The frame rate (expressed in frames per second, fps) and the frame dimensions (width and height) are extracted in order to create an ouput video that will contain the input frames annotated with the detected pose landmarks. This new video will have the same frame rate and frame dimensions as the input video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hn3WxygRrrG"
      },
      "outputs": [],
      "source": [
        "# Read the input video and extract video information\n",
        "input_path = 'my_input_path'+'.mp4'\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "if cap.isOpened() is False:\n",
        "    print('Video not found')\n",
        "else:\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  width = int(cap.get(cv2. CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2. CAP_PROP_FRAME_HEIGHT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n64hMoaeDg7C"
      },
      "source": [
        "We need to create a VideoWriter object to be able to save the output video. The video path - containing the video file extension - should be specified. The VideoWriter object constructor takes the path, the [FourCC](https://https://en.wikipedia.org/wiki/FourCC) that identifies the video format, the frame rate as well as the frame size. An `'MP4V'` FourCC corresponds to the MP4 format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8xREqVoTPtJ"
      },
      "outputs": [],
      "source": [
        "# Create a VideoWriter object\n",
        "output_path = 'my_output_path'+'.mp4'\n",
        "output_vid = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8grwJ4ObGPMF"
      },
      "source": [
        "Next, we set the options needed to create a PoseLandmarker object. Among the options, the downloaded model path and the video mode are indicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XF7ssg7PhSR"
      },
      "outputs": [],
      "source": [
        "# Set options for the PoseLandmarker object\n",
        "from mediapipe.tasks import python\n",
        "\n",
        "model_path = 'my_model_path'+'.task'\n",
        "\n",
        "BaseOptions = python.BaseOptions\n",
        "PoseLandmarker = python.vision.PoseLandmarker\n",
        "PoseLandmarkerOptions = python.vision.PoseLandmarkerOptions\n",
        "VisionRunningMode = python.vision.RunningMode\n",
        "options = PoseLandmarkerOptions(base_options=BaseOptions(model_asset_path=model_path),\n",
        "                                running_mode=VisionRunningMode.VIDEO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyo39pOqJgQ2"
      },
      "source": [
        "Pose landmarks are detected on each video frame by calling the `detect_for_video` method. This function needs the current frame to be converted to an Image object, and takes as second parameter its corresponding frame timestamp in milliseconds ($ms$). The frame timestamp can be obtained as:\n",
        "\n",
        "<center> $ frame\\,timestamp = 1000*\\frac{frame\\,index}{frame\\,rate} $ </center>\n",
        "\n",
        "The frame index is an integer between $1$ and the total number of frames, describing the current frame number. The frame index can be obtained either with a counter variable incremented each time a new frame is read, or using the VideoCapure `get` method as follows:\n",
        "`int(cap.get(cv2.CAP_PROP_POS_FRAMES))`.\n",
        "\n",
        "Then, the `draw_landmarks_on_image` function is called to draw the detected body landmark positions and connections. I use the `cv2_imshow` function to display the annotated frames since I am using Google Colab. If you are using another environment, you can use `cv2.imshow`.\n",
        "\n",
        "Finally, the annotated frame is saved as an output video frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO-F4KeDU_h7"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Extract pose landmarks\n",
        "with PoseLandmarker.create_from_options(options) as landmarker:\n",
        "    frame_index = 0\n",
        "    while cap.isOpened():\n",
        "        hasFrame, image = cap.read()\n",
        "        if not hasFrame:\n",
        "            print('No more frames to read!')\n",
        "            break\n",
        "\n",
        "        # Reorder the RGB color channels as data is loaded with the BGR order with the read method\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # Transform the frame to a NumPy ndarray before converting it to an Image\n",
        "        numpy_frame_from_opencv = np.asarray(image)\n",
        "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)\n",
        "        frame_index += 1 # you can use cap.get(cv2.CAP_PROP_POS_FRAMES) instead\n",
        "        # Compute the frame timestamp and cast it to int as required by the detect_for_video function\n",
        "        frame_timestamp_ms = int(1000*frame_index / fps)\n",
        "        result = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
        "        # Draw the landmarks on the frame as a NumPy ndarray\n",
        "        annotated_image = draw_landmarks_on_image(mp_image.numpy_view(), result)\n",
        "        # Order color channels back to BGR\n",
        "        cv_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "        # Show the annotated frame\n",
        "        cv2_imshow(cv_image)\n",
        "\n",
        "        # Stop reading the video if 'q' key is pressed\n",
        "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "        output_vid.write(cv_image)\n",
        "\n",
        "    cap.release()\n",
        "    output_vid.release()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "In this tutorial we have seen how to use MediaPipe for human pose detection on a video. To this end, we've followed these steps: install MediaPipe, download the pose detection model, load the input video, detect pose landmarks on the input frames and save them on each frame to constitute an ouptut video. I hope you have learnt something new through this tutorial and see you next time!\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/rmeziatisab/PoseDetection/blob/main/goodbye_img.png?raw=true:, width=250\" alt=\"My Image\" width=250>\n",
        "  <p style=\"font-size: 14px;\" align=\"center\">\n",
        "      <i>Goodbye :-) </i>\n",
        "  </p>\n",
        "</p>"
      ],
      "metadata": {
        "id": "psZLB13mZH3E"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}